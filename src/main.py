import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import RobustScaler
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LeakyReLU, Input, BatchNormalization, Dropout
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import joblib
import matplotlib.pyplot as plt
from datetime import datetime
import argparse
import sys
from time import time

# ==== è·¯å¾„ä¸ç¯å¢ƒè®¾ç½® / Path & Environment Setup ====
current_file = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(current_file))
sys.path.insert(0, project_root)
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # æŒ‡å®š GPU è®¾å¤‡ (Set CUDA device)

# ==== è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥ / Import Custom Modules ====
from src.param_calculator import calculate_rolling_periods, calculate_jpm_metrics


# ==========================================================
# 1ï¸âƒ£ åŸºç¡€å‡½æ•° (Monte Carlo Path Simulation & Payoff)
# ==========================================================
def generate_paths(s0, r, q, sigma, T, n_steps, n_paths):
    """ç”Ÿæˆè‚¡ç¥¨ä»·æ ¼è·¯å¾„ (å‡ ä½•å¸ƒæœ—è¿åŠ¨)."""
    dt = T / n_steps
    z = np.random.normal(0, 1, (n_paths, n_steps))
    increments = (r - q - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * z
    log_paths = np.cumsum(increments, axis=1)
    log_paths = np.hstack([np.zeros((n_paths, 1)), log_paths])
    return s0 * np.exp(log_paths)


def asian_option_value(paths, K, r, T, option_type='call', start_step=0):
    """è®¡ç®—äºšæ´²æœŸæƒä»·å€¼ï¼ˆå¹³å‡ä»·æ ¼æœŸæƒï¼‰."""
    averages = np.mean(paths[:, start_step:], axis=1)
    if option_type.lower() == 'call':
        payoffs = np.maximum(averages - K, 0)
    else:
        payoffs = np.maximum(K - averages, 0)

    remaining_time = T * (paths.shape[1] - 1 - start_step) / (paths.shape[1] - 1)
    return np.exp(-r * remaining_time) * payoffs


def generate_training_data(s0, K, r, q, sigma, T, choice_date, n_steps, n_paths, option_type='asian'):
    """ç”Ÿæˆè®­ç»ƒæ ·æœ¬ (è·¯å¾„ + payoff + å‚æ•°çŸ©é˜µ)."""
    paths = generate_paths(s0, r, q, sigma, T, n_steps, n_paths)
    choice_step = int(choice_date / T * n_steps)

    if option_type == 'asian':
        call_values = asian_option_value(paths, K, r, T, 'call', start_step=choice_step)
        put_values = asian_option_value(paths, K, r, T, 'put', start_step=choice_step)
    else:
        call_values = asian_option_value(paths, K, r, T, 'call', start_step=choice_step)
        put_values = asian_option_value(paths, K, r, T, 'put', start_step=choice_step)

    final_payoffs = np.maximum(call_values, put_values)
    choice_frac = choice_date / T
    params_array = np.tile(np.array([s0, K, r, q, sigma, T, choice_frac], dtype=np.float32), (n_paths, 1))
    return paths, final_payoffs, params_array


# ==========================================================
# 2ï¸âƒ£ ç‰¹å¾å·¥ç¨‹æ¨¡å—
# ==========================================================
def create_robust_features(paths, s0, K, params_array):
    """æå–é²æ£’æ€§ç‰¹å¾ï¼ŒåŒ…æ‹¬ç»Ÿè®¡ç‰¹å¾ã€åŠ¨é‡æŒ‡æ ‡å’Œæ—¶é—´åŠ æƒä»·æ ¼."""
    n_samples, n_steps = paths.shape

    # --- åŸºç¡€ä»·æ ¼ç‰¹å¾ ---
    current_price = paths[:, -1] / s0
    moneyness = current_price / (K / s0)
    log_moneyness = np.log(np.clip(moneyness, 1e-6, 1e6))

    # --- è·¯å¾„ç»Ÿè®¡ ---
    mean_price = np.mean(paths, axis=1) / s0
    max_price = np.max(paths, axis=1) / s0
    min_price = np.min(paths, axis=1) / s0

    # --- å®ç°æ³¢åŠ¨ç‡ ---
    path_returns = np.diff(np.log(np.clip(paths, 1e-6, None)), axis=1)
    realized_vol = np.std(path_returns, axis=1) * np.sqrt(252)

    # --- æŠ€æœ¯æŒ‡æ ‡ ---
    lookback_short = min(10, n_steps)
    sma_short = np.mean(paths[:, -lookback_short:], axis=1) / s0
    sma_long = np.mean(paths, axis=1) / s0
    momentum = (paths[:, -1] - paths[:, max(0, n_steps - lookback_short)]) / paths[:, max(0, n_steps - lookback_short)]

    # --- æœ€å¤§å›æ’¤ ---
    running_max = np.maximum.accumulate(paths, axis=1)
    drawdowns = (running_max - paths) / (running_max + 1e-8)
    max_drawdown = np.max(drawdowns, axis=1)

    # --- æ—¶é—´åŠ æƒå‡ä»· ---
    weights = np.linspace(0.1, 1.0, n_steps)
    time_weighted_avg = np.average(paths, axis=1, weights=weights) / s0

    # --- å…³é”®æ—¶é—´ç‚¹ä»·æ ¼ ---
    if n_steps >= 5:
        time_indices = [0, n_steps // 4, n_steps // 2, 3 * n_steps // 4, n_steps - 1]
        key_prices = paths[:, time_indices] / s0
    else:
        key_prices = paths / s0

    # --- æ±‡æ€»æ‰€æœ‰ç‰¹å¾ ---
    features = np.column_stack([
        current_price, moneyness, log_moneyness, mean_price,
        max_price, min_price, realized_vol, sma_short,
        sma_long, momentum, max_drawdown, time_weighted_avg,
        key_prices, params_array
    ])

    return features.astype(np.float32)


# ==========================================================
# 3ï¸âƒ£ æ•°æ®ç”Ÿæˆç­–ç•¥
# ==========================================================

def generate_training_data_with_modes(n_total_paths=400000, 
                                     banks=['GS', 'BAC', 'WFC', 'C', 'MS'],
                                     data_mode='mixed',
                                     enhanced=False):
    """
    ç²¾ç®€ç‰ˆå¤šé“¶è¡Œæ•°æ®ç”Ÿæˆå™¨
    
    å‚æ•°:
    n_total_paths: æ€»è·¯å¾„æ•°
    banks: é“¶è¡Œåˆ—è¡¨
    data_mode: æ•°æ®æ¨¡å¼ - 'historical_only', 'synthetic_only', 'mixed'
    enhanced: æ˜¯å¦ä½¿ç”¨å¢å¼ºå†å²æ•°æ®ï¼ˆæ·»åŠ æ‰°åŠ¨ï¼‰
    """
    n_batches = 100
    paths_per_batch = n_total_paths // n_batches
    
    # ç¡®å®šæ‰¹æ¬¡åˆ†é…
    if data_mode == 'historical_only':
        hist_batches, rand_batches = 100, 0
    elif data_mode == 'synthetic_only':
        hist_batches, rand_batches = 0, 100
    else:  # mixed
        hist_batches, rand_batches = 50, 50
    
    print(f"æ¨¡å¼: {data_mode}, å†å²æ‰¹æ¬¡: {hist_batches}, éšæœºæ‰¹æ¬¡: {rand_batches}")
    
    all_X, all_y, all_params = [], [], []
    
    # åŠ è½½å†å²æ•°æ®
    historical_batches = []
    if hist_batches > 0:
        for bank in banks:
            try:
                data = calculate_rolling_periods(bank)
                if not data.empty:
                    # ç®€å•é‡‡æ ·ï¼Œæ¯ä¸ªé“¶è¡Œè‡³å°‘åˆ†é…ä¸€äº›æ‰¹æ¬¡
                    samples = data.sample(min(len(data), hist_batches//len(banks)+1), 
                                         replace=True, random_state=42)
                    for _, row in samples.iterrows():
                        historical_batches.append((bank, row))
                    print(f"âœ… {bank}: {len(samples)}æ¡æ•°æ®")
            except Exception as e:
                print(f"âŒ {bank}åŠ è½½å¤±è´¥: {e}")
    
    # ç”Ÿæˆå†å²æ•°æ®æ‰¹æ¬¡
    for i in range(min(hist_batches, len(historical_batches))):
        bank, row = historical_batches[i]
        
        if enhanced:
            # å¢å¼ºæ¨¡å¼ï¼šæ·»åŠ æ‰°åŠ¨
            s0 = float(row['s0']) * np.random.uniform(0.95, 1.05)
            sigma = max(0.05, float(row['sigma']) * np.random.uniform(0.9, 1.1))
            q = max(0.0, float(row['q']) * np.random.uniform(0.8, 1.2))
            r = max(0.001, float(row['r']) * np.random.uniform(0.9, 1.1))
        else:
            # æ™®é€šæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨å†å²æ•°æ®
            s0 = float(row['s0'])
            sigma = float(row['sigma'])
            q = float(row['q'])
            r = float(row['r'])
        
        # éšæœºåŒ–å…¶ä»–å‚æ•°
        K = s0 * np.random.uniform(0.9, 1.1)
        T = float(np.random.uniform(0.8, 1.2))
        choice_date = T * np.random.uniform(0.4, 0.6)
        
        params = dict(s0=s0, K=K, r=r, q=q, sigma=sigma, T=T,
                      n_steps=252, n_paths=paths_per_batch,
                      option_type='asian', choice_date=choice_date)
        
        try:
            paths, payoffs, params_array = generate_training_data(**params)
            features = create_robust_features(paths, s0, K, params_array)
            all_X.append(features)
            all_y.append(payoffs)
            all_params.append(params_array)
        except Exception as e:
            print(f"å†å²æ‰¹æ¬¡ {i} å¤±è´¥: {e}")
            continue
    
    # ç”Ÿæˆéšæœºæ•°æ®æ‰¹æ¬¡
    for i in range(rand_batches + max(0, hist_batches - len(historical_batches))):
        s0 = float(np.random.uniform(90, 250))
        K = s0 * np.random.uniform(0.9, 1.1)
        r = float(np.random.uniform(0.0001, 0.055))
        
        # éšæœºè‚¡æ¯ç‡
        rand = np.random.random()
        if rand < 0.1:
            q = 0.0
        elif rand < 0.7:
            q = float(np.random.uniform(0.0, 0.04))
        else:
            q = float(np.random.uniform(0.04, 0.1))
            
        sigma = float(np.random.uniform(0.1, 0.5))
        T = float(np.random.uniform(0.8, 1.2))
        choice_date = T * np.random.uniform(0.4, 0.6)
        
        params = dict(s0=s0, K=K, r=r, q=q, sigma=sigma, T=T,
                      n_steps=252, n_paths=paths_per_batch,
                      option_type='asian', choice_date=choice_date)
        
        try:
            paths, payoffs, params_array = generate_training_data(**params)
            features = create_robust_features(paths, s0, K, params_array)
            all_X.append(features)
            all_y.append(payoffs)
            all_params.append(params_array)
        except Exception as e:
            print(f"éšæœºæ‰¹æ¬¡ {i} å¤±è´¥: {e}")
            continue

    if not all_X:
        raise ValueError("æœªæˆåŠŸç”Ÿæˆä»»ä½•æ•°æ®")

    X = np.concatenate(all_X)
    y = np.concatenate(all_y)
    params = np.concatenate(all_params)
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {X.shape[0]} æ¡æ ·æœ¬")
    return X, y, params

# ==========================================================
# 4ï¸âƒ£ è‡ªå®šä¹‰å˜æ¢ä¸æŸå¤±å‡½æ•°
# ==========================================================
class SignedLogTransform:
    """å¯¹æ•°ç¬¦å·å˜æ¢: T(y)=sign(y)*log(1+|y|/c)"""
    def __init__(self, c=1.0):
        self.c = float(c)
    def transform(self, y):
        return tf.sign(y) * tf.math.log(1.0 + tf.abs(y) / self.c)
    def inverse(self, z):
        return tf.sign(z) * (self.c * (tf.math.exp(tf.abs(z)) - 1.0))
    def log_abs_det_jacobian(self, y):
        return -tf.math.log(self.c + tf.abs(y) + 1e-8)


def gaussian_nll_transformed(transform):
    """é«˜æ–¯è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (å¸¦å˜æ¢)."""
    def loss(y_true, y_pred):
        y_true = tf.reshape(y_true, (-1,))
        mu, log_sigma = y_pred[:, 0], y_pred[:, 1]
        sigma = tf.nn.softplus(log_sigma) + 1e-6
        z = transform.transform(y_true)
        nll_z = 0.5 * (tf.math.log(2.0 * np.pi) + 2.0 * tf.math.log(sigma) + tf.square((z - mu) / sigma))
        log_jac = transform.log_abs_det_jacobian(y_true)
        return tf.reduce_mean(nll_z - log_jac)
    return loss


def mae_mu(transform):
    """MAE (é€†å˜æ¢åçš„å‡å€¼é¢„æµ‹è¯¯å·®)."""
    def metric(y_true, y_pred):
        mu_z = y_pred[:, 0]
        y_hat = transform.inverse(mu_z)
        return tf.reduce_mean(tf.abs(y_true - y_hat))
    return metric


def mse_mu(transform):
    """MSE (é€†å˜æ¢åçš„å‡å€¼é¢„æµ‹è¯¯å·®)."""
    def metric(y_true, y_pred):
        mu_z = y_pred[:, 0]
        y_hat = transform.inverse(mu_z)
        return tf.reduce_mean(tf.square(y_true - y_hat))
    return metric


# ==========================================================
# 5ï¸âƒ£ æ¨¡å‹ç»“æ„å®šä¹‰
# ==========================================================
def build_improved_mlp(input_dim):
    """æ„å»ºæ”¹è¿›ç‰ˆ MLP æ¨¡å‹ (è¾“å‡ºÎ¼ä¸logÏƒ)."""
    model = Sequential([
        Input(shape=(input_dim,)),
        Dense(256), BatchNormalization(), LeakyReLU(0.1), Dropout(0.05),
        Dense(256), BatchNormalization(), LeakyReLU(0.1), Dropout(0.05),
        Dense(128), BatchNormalization(), LeakyReLU(0.1),
        Dense(2, activation='linear')
    ])
    return model


# ==========================================================
# 6ï¸âƒ£ è®­ç»ƒå‡½æ•°
# ==========================================================
def train_model(data_type='mixed', enhanced=True, n_total_paths=400000):
    """è®­ç»ƒæ¨¡å‹çš„ä¸»å‡½æ•°"""
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    physical_devices = tf.config.list_physical_devices('GPU')
    if physical_devices:
        tf.config.experimental.set_memory_growth(physical_devices[0], True)
        print("âœ… ä½¿ç”¨ GPU è®­ç»ƒ")
    else:
        print("âš™ï¸ ä½¿ç”¨ CPU è®­ç»ƒ")

    try:
        print(f"\n=== Step 1: æ•°æ®ç”Ÿæˆ (æ¨¡å¼: {data_type}, å¢å¼º: {enhanced}) ===")
        X, y, params = generate_training_data_with_modes(
            n_total_paths=n_total_paths, 
            data_mode=data_type,
            enhanced=enhanced
        )

        print("\n=== Step 2: æ•°æ®é›†åˆ’åˆ† ===")
        X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)
        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42)

        print("\n=== Step 3: ç‰¹å¾æ ‡å‡†åŒ– ===")
        scaler = RobustScaler()
        X_train_s = scaler.fit_transform(X_train)
        X_val_s = scaler.transform(X_val)
        X_test_s = scaler.transform(X_test)

        print("\n=== Step 4: æ„å»ºä¸ç¼–è¯‘æ¨¡å‹ ===")
        transform = SignedLogTransform()
        model = build_improved_mlp(X_train_s.shape[1])
        model.compile(
            optimizer=Adam(learning_rate=5e-5, amsgrad=True, clipnorm=1.0),
            loss=gaussian_nll_transformed(transform),
            metrics=[mae_mu(transform), mse_mu(transform)]
        )

        callbacks = [
            EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, min_delta=5e-5),
            ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=10, min_lr=5e-7, verbose=1)
        ]

        print("\n=== Step 5: æ¨¡å‹è®­ç»ƒ ===")
        timestamp = datetime.now().strftime("%m%d_%H%M")

        history = model.fit(
            X_train_s, y_train,
            validation_data=(X_val_s, y_val),
            epochs=400,
            batch_size=512,
            shuffle=True,
            verbose=1,
            callbacks=callbacks
        )

        print("\n=== Step 6: æ¨¡å‹è¯„ä¼° ===")
        y_pred = model.predict(X_test_s)
        y_pred = transform.inverse(y_pred[:, 0]).numpy()

        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"\nâœ… Test MSE: {mse:.6f}, RÂ²: {r2:.4f}")

        # ä¿å­˜æ¨¡å‹ä¸Scaler
        os.makedirs('scalers', exist_ok=True)
        os.makedirs('models', exist_ok=True)
        
        joblib.dump(scaler, f'scalers/X_scaler_{data_type}_{timestamp}.pkl')
        model.save(f'models/chooser_option_mlp_model_{data_type}_{timestamp}.h5')
        print("ğŸ“ æ¨¡å‹ä¸Scalerå·²ä¿å­˜")

        # ç»˜åˆ¶Lossæ›²çº¿
        plt.figure(figsize=(10, 6))
        plt.plot(history.history['loss'], label='train')
        plt.plot(history.history['val_loss'], label='val')
        plt.yscale('log')
        plt.title(f"Data: {data_type}, Enhanced: {enhanced}\nRÂ²={r2:.4f}, true_mean={y_test.mean():.4f}, pred_mean={y_pred.mean():.4f}")
        plt.legend()
        plt.show()

        return model, scaler, history

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

# ==========================================================
# 7ï¸âƒ£ è¯„ä¼°å‡½æ•°
# ==========================================================
def evaluate_model(model_path, data_type='mixed', n_paths=500000):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ä¸»å‡½æ•°"""
    print(f"ğŸ¯ å¼€å§‹è¯„ä¼°æ¨¡å¼ - æ¨¡å‹: {model_path}")
    
    # ==== åŸºç¡€å‚æ•°è®¾ç½® / Base Parameters ====
    base_params = {
        's0': 156.7,           # åˆå§‹è‚¡ä»· / Initial stock price
        'K': 150,              # æ‰§è¡Œä»· / Strike price
        'r': 0.0015,           # æ— é£é™©åˆ©ç‡ / Risk-free rate
        'q': 0.0233,           # è‚¡æ¯ç‡ / Dividend yield
        'sigma': 0.282,        # æ³¢åŠ¨ç‡ / Volatility
        'T': 1.0,              # åˆ°æœŸæ—¶é—´ / Time to maturity
        'choice_date': 0.5,    # é€‰æ‹©æ—¥æœŸ / Choice date (e.g., half-year)
        'n_steps': 252,        # æ—¶é—´æ­¥æ•° / Number of time steps
        'n_paths': n_paths,    # æ¨¡æ‹Ÿè·¯å¾„æ•° / Number of Monte Carlo paths
        'option_type': 'asian' # æœŸæƒç±»å‹ / Option type ('asian' or 'lookback')
    }

    # ==== åŠ¨æ€æ›´æ–°å‚æ•° / Update Parameters Dynamically ====
    try:
        params = calculate_jpm_metrics('2021-8-23', '2022-8-23')
        params['K'] = round(params['s0'], -1)
        base_params.update(params)
        print("âœ… JPMå‚æ•°æ›´æ–°æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ JPMå‚æ•°æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°: {e}")

    # ==== Step 1: ç”Ÿæˆ Monte Carlo æ•°æ® / Generate Monte Carlo Data ====
    print("\n=== Step 1: ç”Ÿæˆ Monte Carlo æ•°æ® ===")
    t1 = time()
    paths, payoffs, params_array = generate_training_data(**base_params)
    t2 = time()

    # ==== åŠ è½½æ¨¡å‹ä¸Scaler / Load Model & Scaler ====
    print("\n=== Step 2: åŠ è½½æ¨¡å‹ä¸Scaler ===")
    try:
        print("å¼€å§‹åŠ è½½æ¨¡å‹...")
        model = load_model(model_path, compile=False)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

        # ä»æ¨¡å‹è·¯å¾„æ¨æ–­scalerè·¯å¾„ - ä¿®æ”¹è¿™éƒ¨åˆ†
        scaler_dir = 'scalers'
        model_filename = os.path.basename(model_path)
        
        # å°è¯•å¤šç§å¯èƒ½çš„scalerå‘½åè§„åˆ™
        possible_scaler_paths = [
            # è§„åˆ™1: ç›´æ¥æ›¿æ¢æ¨¡å‹åä¸ºscalerå
            model_path.replace('models/', 'scalers/').replace('_model_', '_scaler_').replace('.h5', '.pkl'),
            # è§„åˆ™2: ä½¿ç”¨å›ºå®šçš„scaleræ–‡ä»¶åï¼ˆæ ¹æ®ä½ çš„å®é™…æ–‡ä»¶åï¼‰
            os.path.join(scaler_dir, 'X_scaler_mixed.pkl'),
            # è§„åˆ™3: ä»æ¨¡å‹åæå–æ•°æ®ç±»å‹
            os.path.join(scaler_dir, f"X_scaler_{data_type}.pkl"),
            # è§„åˆ™4: ç®€å•çš„æ–‡ä»¶åæ›¿æ¢
            model_path.replace('.h5', '.pkl').replace('models', 'scalers')
        ]
        
        X_scaler = None
        used_path = None
        
        for scaler_path in possible_scaler_paths:
            if os.path.exists(scaler_path):
                X_scaler = joblib.load(scaler_path)
                used_path = scaler_path
                print(f"âœ… ScaleråŠ è½½æˆåŠŸ: {used_path}")
                break
        
        if X_scaler is None:
            # å¦‚æœæ‰€æœ‰è§„åˆ™éƒ½å¤±è´¥ï¼Œåˆ—å‡ºå¯ç”¨çš„scaleræ–‡ä»¶
            available_scalers = [f for f in os.listdir(scaler_dir) if f.endswith('.pkl')]
            print(f"âŒ æ— æ³•è‡ªåŠ¨æ‰¾åˆ°å¯¹åº”çš„scaleræ–‡ä»¶")
            print(f"ğŸ“ å¯ç”¨çš„scaleræ–‡ä»¶: {available_scalers}")
            raise FileNotFoundError("è¯·æ‰‹åŠ¨æŒ‡å®šæ­£ç¡®çš„scaleræ–‡ä»¶è·¯å¾„")
            
    except Exception as e:
        print(f"âŒ åŠ è½½è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

    print("âœ… æ¨¡å‹ä¸ScaleråŠ è½½å®Œæˆ")

    # ==== ç‰¹å¾æ„å»º / Feature Construction ====
    print("\n=== Step 3: ç‰¹å¾å·¥ç¨‹ ===")
    base_params_stack = np.tile(
        np.array([
            base_params['s0'],
            base_params['K'],
            base_params['r'],
            base_params['q'],
            base_params['sigma'],
            base_params['T'],
            base_params['choice_date'] / base_params['T']
        ], dtype=np.float32),
        (base_params['n_paths'], 1)
    )

    X_enhanced = create_robust_features(paths, base_params['s0'], base_params['K'], base_params_stack)

    # ==== ç‰¹å¾ç¼©æ”¾ / Feature Scaling ====
    X_test_scaled = X_scaler.transform(X_enhanced)

    # ==== æ¨¡å‹é¢„æµ‹ / Model Prediction ====
    print("\n=== Step 4: æ¨¡å‹é¢„æµ‹ ===")
    y_pred_transformed = model.predict(X_test_scaled)

    # å…¼å®¹å¤šè¾“å‡ºæ¨¡å‹ / Handle different output formats
    if y_pred_transformed.ndim == 2 and y_pred_transformed.shape[1] == 2:
        y_pred = y_pred_transformed[:, 0]
    else:
        y_pred = y_pred_transformed.reshape(-1)

    # ==== é€†å˜æ¢é¢„æµ‹ç»“æœ / Inverse Transform ====
    transform = SignedLogTransform()
    y_pred = transform.inverse(y_pred)
    y_pred = y_pred.numpy()

    t3 = time()

    print(f"â±ï¸ è’™ç‰¹å¡æ´›è€—æ—¶: {t2 - t1:.4f}s")
    print(f"â±ï¸ æ¨¡å‹é¢„æµ‹è€—æ—¶: {t3 - t2:.4f}s")

    # ==== æ€§èƒ½æŒ‡æ ‡è®¡ç®— / Compute Metrics ====
    print("\n=== Step 5: æ€§èƒ½è¯„ä¼° ===")
    y_test = payoffs
    print(f"ğŸ“Š y_test mean: {y_test.mean():.4f}, std: {y_test.std():.4f}")
    print(f"ğŸ“Š y_pred mean: {y_pred.mean():.4f}, std: {y_pred.std():.4f}")

    mae = np.mean(np.abs(y_test - y_pred))
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)

    print(f"âœ… MAE: {mae:.4f}")
    print(f"âœ… RMSE: {rmse:.4f}")
    print(f"âœ… RÂ²: {r2:.4f}")

    # ==== æç«¯å€¼è¯¯å·®åˆ†æ / Extreme Value Error Analysis ====
    def analyze_extremes(y_true, y_pred, percentile=90):
        """è®¡ç®—æç«¯æ ·æœ¬ä¸æ­£å¸¸æ ·æœ¬çš„è¯¯å·® / Compare errors for extreme vs normal samples."""
        threshold = np.percentile(y_true, percentile)
        mask_extreme = y_true > threshold
        extreme_mae = np.mean(np.abs(y_true[mask_extreme] - y_pred[mask_extreme]))
        normal_mae = np.mean(np.abs(y_true[~mask_extreme] - y_pred[~mask_extreme]))
        return extreme_mae, normal_mae

    extreme_mae, normal_mae = analyze_extremes(y_test, y_pred)
    print(f"ğŸ“ˆ æç«¯å€¼MAE: {extreme_mae:.4f}")
    print(f"ğŸ“‰ æ­£å¸¸å€¼MAE: {normal_mae:.4f}")

    # ==== å¯è§†åŒ–åˆ†æ / Visualization ====
    print("\n=== Step 6: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # --- å·¦å›¾ï¼šé¢„æµ‹å€¼ vs è’™ç‰¹å¡æ´›çœŸå€¼ / Left: Predicted vs True ---
    ax1 = axes[0]
    ax1.scatter(y_test, y_pred, alpha=0.25, s=8)
    lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]
    ax1.plot(lims, lims, 'r--', lw=1)
    ax1.set_xlabel("Monte Carlo Payoff (True)")
    ax1.set_ylabel("ML Predicted Payoff")
    ax1.set_title("Chooser Option: Prediction vs Monte Carlo")
    ax1.axis('equal')
    ax1.grid(alpha=0.3)

    # --- ä¸­å›¾ï¼šè¯¯å·®åˆ†å¸ƒ / Middle: Error Distribution ---
    ax2 = axes[1]
    errors = y_pred - y_test
    ax2.hist(errors, bins=50, alpha=0.7, color='steelblue')
    ax2.set_xlabel("Prediction Error (Pred - True)")
    ax2.set_ylabel("Frequency")
    ax2.set_title("Residual Distribution")
    ax2.grid(alpha=0.3)

    # --- å³å›¾ï¼šé¢„æµ‹åˆ†å¸ƒ vs çœŸå€¼åˆ†å¸ƒ / Right: Value Distribution ---
    ax3 = axes[2]
    ax3.hist(y_test, bins=50, alpha=0.7, label='True Values', color='blue')
    ax3.hist(y_pred, bins=50, alpha=0.7, label='Predicted Values', color='orange')
    ax3.legend()
    ax3.set_xlabel('Value')
    ax3.set_ylabel('Frequency')
    ax3.set_title('Distribution Comparison')
    ax3.grid(alpha=0.3)

    # --- å›¾æ ‡é¢˜ä¸å¸ƒå±€ / Overall Title & Layout ---
    fig.suptitle(
        f'Chooser Option MLP Evaluation (MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f})',
        fontsize=14,
        fontweight='bold'
    )

    plt.tight_layout()    
    plt.show()

    return {
        'mae': mae,
        'rmse': rmse, 
        'r2': r2,
        'extreme_mae': extreme_mae,
        'normal_mae': normal_mae,
        'mc_time': t2 - t1,
        'inference_time': t3 - t2
    }

# ==========================================================
# 8ï¸âƒ£ é¢„æµ‹å‡½æ•°
# ==========================================================
def predict_option_price(model_path, s0, K, r, q, sigma, T, choice_date, n_paths=100000):
    """é¢„æµ‹é€‰æ‹©æœŸæƒä»·æ ¼çš„ä¸»å‡½æ•°"""
    print(f"ğŸ”® å¼€å§‹é¢„æµ‹æ¨¡å¼")
    print(f"ğŸ“Š è¾“å…¥å‚æ•°:")
    print(f"  s0 (åˆå§‹è‚¡ä»·): {s0}")
    print(f"  K (æ‰§è¡Œä»·): {K}")
    print(f"  r (æ— é£é™©åˆ©ç‡): {r}")
    print(f"  q (è‚¡æ¯ç‡): {q}")
    print(f"  sigma (æ³¢åŠ¨ç‡): {sigma}")
    print(f"  T (åˆ°æœŸæ—¶é—´): {T}")
    print(f"  choice_date (é€‰æ‹©æ—¥æœŸ): {choice_date}")
    print(f"  n_paths (æ¨¡æ‹Ÿè·¯å¾„æ•°): {n_paths}")

    # ==== å‚æ•°éªŒè¯ ====
    if choice_date >= T:
        raise ValueError(f"é€‰æ‹©æ—¥æœŸ ({choice_date}) å¿…é¡»å°äºåˆ°æœŸæ—¶é—´ ({T})")
    
    if s0 <= 0 or K <= 0 or T <= 0:
        raise ValueError("è‚¡ä»·ã€æ‰§è¡Œä»·å’Œåˆ°æœŸæ—¶é—´å¿…é¡»ä¸ºæ­£æ•°")

    # ==== åŠ è½½æ¨¡å‹ä¸Scaler ====
    print("\n=== Step 1: åŠ è½½æ¨¡å‹ä¸Scaler ===")
    try:
        model = load_model(model_path, compile=False)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

        # ä»æ¨¡å‹è·¯å¾„æ¨æ–­scalerè·¯å¾„ - ä¿®æ”¹è¿™éƒ¨åˆ†
        scaler_dir = 'scalers'
        model_filename = os.path.basename(model_path)
        
        # å°è¯•å¤šç§å¯èƒ½çš„scalerå‘½åè§„åˆ™
        possible_scaler_paths = [
            # ä½¿ç”¨ä½ æä¾›çš„å›ºå®šæ–‡ä»¶å
            os.path.join(scaler_dir, 'X_scaler_mixed.pkl'),
            # å…¶ä»–å¯èƒ½çš„å‘½åè§„åˆ™
            model_path.replace('models/', 'scalers/').replace('_model_', '_scaler_').replace('.h5', '.pkl'),
            model_path.replace('.h5', '.pkl').replace('models', 'scalers')
        ]
        
        X_scaler = None
        used_path = None
        
        for scaler_path in possible_scaler_paths:
            if os.path.exists(scaler_path):
                X_scaler = joblib.load(scaler_path)
                used_path = scaler_path
                print(f"âœ… ScaleråŠ è½½æˆåŠŸ: {used_path}")
                break
        
        if X_scaler is None:
            # å¦‚æœæ‰€æœ‰è§„åˆ™éƒ½å¤±è´¥ï¼Œåˆ—å‡ºå¯ç”¨çš„scaleræ–‡ä»¶
            available_scalers = [f for f in os.listdir(scaler_dir) if f.endswith('.pkl')]
            print(f"âŒ æ— æ³•è‡ªåŠ¨æ‰¾åˆ°å¯¹åº”çš„scaleræ–‡ä»¶")
            print(f"ğŸ“ å¯ç”¨çš„scaleræ–‡ä»¶: {available_scalers}")
            raise FileNotFoundError("è¯·æ‰‹åŠ¨æŒ‡å®šæ­£ç¡®çš„scaleræ–‡ä»¶è·¯å¾„")

    except Exception as e:
        print(f"âŒ åŠ è½½è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==== ç”Ÿæˆæµ‹è¯•æ•°æ® ====
    print("\n=== Step 2: ç”Ÿæˆæµ‹è¯•æ•°æ® ===")
    try:
        t1 = time()
        paths, payoffs, params_array = generate_training_data(
            s0=s0, K=K, r=r, q=q, sigma=sigma, T=T,
            choice_date=choice_date, n_steps=252, n_paths=n_paths,
            option_type='asian'
        )
        t2 = time()
        print(f"âœ… æˆåŠŸç”Ÿæˆ {n_paths} æ¡è·¯å¾„ï¼Œè€—æ—¶: {t2-t1:.2f}s")
    except Exception as e:
        print(f"âŒ æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
        return None

    # ==== ç‰¹å¾å·¥ç¨‹ ====
    print("\n=== Step 3: ç‰¹å¾å·¥ç¨‹ ===")
    base_params_stack = np.tile(
        np.array([s0, K, r, q, sigma, T, choice_date/T], dtype=np.float32),
        (n_paths, 1)
    )

    X_enhanced = create_robust_features(paths, s0, K, base_params_stack)

    # ==== ç‰¹å¾ç¼©æ”¾ ====
    X_scaled = X_scaler.transform(X_enhanced)

    # ==== æ¨¡å‹é¢„æµ‹ ====
    print("\n=== Step 4: æ¨¡å‹é¢„æµ‹ ===")
    try:
        y_pred_transformed = model.predict(X_scaled, batch_size=1024, verbose=1)
        
        # å¤„ç†æ¨¡å‹è¾“å‡º
        if y_pred_transformed.ndim == 2 and y_pred_transformed.shape[1] == 2:
            y_pred = y_pred_transformed[:, 0]
        else:
            y_pred = y_pred_transformed.reshape(-1)

        # é€†å˜æ¢
        transform = SignedLogTransform()
        y_pred_payoffs = transform.inverse(y_pred).numpy()
        
        t3 = time()
        print(f"âœ… é¢„æµ‹å®Œæˆï¼Œè€—æ—¶: {t3-t2:.2f}s")

    except Exception as e:
        print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
        return None

    # ==== ç»“æœè®¡ç®— ====
    print("\n=== Step 5: ç»“æœè®¡ç®— ===")
    
    # è®¡ç®—è’™ç‰¹å¡æ´›åŸºå‡†ä»·æ ¼
    mc_price = np.mean(payoffs)
    mc_std = np.std(payoffs) / np.sqrt(n_paths)
    
    # è®¡ç®—æ¨¡å‹é¢„æµ‹ä»·æ ¼
    ml_price = np.mean(y_pred_payoffs)
    ml_std = np.std(y_pred_payoffs) / np.sqrt(n_paths)
    
    # è®¡ç®—é¢„æµ‹åŒºé—´
    confidence = 0.95
    z_score = 1.96  # 95% ç½®ä¿¡åŒºé—´
    
    mc_ci_lower = mc_price - z_score * mc_std
    mc_ci_upper = mc_price + z_score * mc_std
    
    ml_ci_lower = ml_price - z_score * ml_std
    ml_ci_upper = ml_price + z_score * ml_std
    
    # è®¡ç®—ç›¸å¯¹è¯¯å·®
    relative_error = abs(ml_price - mc_price) / mc_price * 100

    # ==== è¾“å‡ºç»“æœ ====
    print("\n" + "="*60)
    print("ğŸ¯ é€‰æ‹©æœŸæƒå®šä»·ç»“æœ")
    print("="*60)
    print(f"ğŸ“Š è’™ç‰¹å¡æ´›åŸºå‡†ä»·æ ¼: {mc_price:.6f}")
    print(f"  95% ç½®ä¿¡åŒºé—´: [{mc_ci_lower:.6f}, {mc_ci_upper:.6f}]")
    print(f"  æ ‡å‡†è¯¯å·®: {mc_std:.6f}")
    print()
    print(f"ğŸ¤– æœºå™¨å­¦ä¹ é¢„æµ‹ä»·æ ¼: {ml_price:.6f}")
    print(f"  95% ç½®ä¿¡åŒºé—´: [{ml_ci_lower:.6f}, {ml_ci_upper:.6f}]")
    print(f"  æ ‡å‡†è¯¯å·®: {ml_std:.6f}")
    print()
    print(f"ğŸ“ˆ ç›¸å¯¹è¯¯å·®: {relative_error:.4f}%")
    print(f"â±ï¸ æ€»è€—æ—¶: {t3-t1:.2f}s")
    print("="*60)

    # ==== å¯è§†åŒ–ç»“æœ ====
    print("\n=== Step 6: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===")
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # å·¦ä¸Šï¼šä»·æ ¼åˆ†å¸ƒå¯¹æ¯”
    ax1 = axes[0, 0]
    ax1.hist(payoffs, bins=50, alpha=0.7, label='Monte Carlo', color='blue', density=True)
    ax1.hist(y_pred_payoffs, bins=50, alpha=0.7, label='ML Prediction', color='orange', density=True)
    ax1.axvline(mc_price, color='blue', linestyle='--', linewidth=2, label=f'MC Mean: {mc_price:.4f}')
    ax1.axvline(ml_price, color='orange', linestyle='--', linewidth=2, label=f'ML Mean: {ml_price:.4f}')
    ax1.set_xlabel('Option Payoff')
    ax1.set_ylabel('Density')
    ax1.set_title('Payoff Distribution Comparison')
    ax1.legend()
    ax1.grid(alpha=0.3)
    
    # å³ä¸Šï¼šé¢„æµ‹vsçœŸå®æ•£ç‚¹å›¾
    ax2 = axes[0, 1]
    ax2.scatter(payoffs, y_pred_payoffs, alpha=0.5, s=10)
    lims = [min(payoffs.min(), y_pred_payoffs.min()), max(payoffs.max(), y_pred_payoffs.max())]
    ax2.plot(lims, lims, 'r--', alpha=0.8, label='Perfect Prediction')
    ax2.set_xlabel('Monte Carlo Payoff')
    ax2.set_ylabel('ML Predicted Payoff')
    ax2.set_title('Prediction vs True Payoff')
    ax2.legend()
    ax2.grid(alpha=0.3)
    
    # å·¦ä¸‹ï¼šä»·æ ¼å¯¹æ¯”æ¡å½¢å›¾
    ax3 = axes[1, 0]
    methods = ['Monte Carlo', 'ML Prediction']
    prices = [mc_price, ml_price]
    errors = [mc_std * z_score, ml_std * z_score]
    bars = ax3.bar(methods, prices, yerr=errors, capsize=10, alpha=0.7, 
                   color=['blue', 'orange'], edgecolor='black')
    ax3.set_ylabel('Option Price')
    ax3.set_title('Price Comparison with 95% Confidence Intervals')
    ax3.grid(alpha=0.3, axis='y')
    
    # åœ¨æ¡å½¢ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, price in zip(bars, prices):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + errors[0],
                f'{price:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # å³ä¸‹ï¼šå‚æ•°æ‘˜è¦
    ax4 = axes[1, 1]
    ax4.axis('off')
    param_text = (
        f"Parameters Summary:\n\n"
        f"Sâ‚€ = {s0:.2f}\n"
        f"K = {K:.2f}\n"
        f"r = {r:.4f}\n"
        f"q = {q:.4f}\n"
        f"Ïƒ = {sigma:.4f}\n"
        f"T = {T:.2f}\n"
        f"choice_date = {choice_date:.2f}\n"
        f"n_paths = {n_paths:,}\n\n"
        f"Results:\n"
        f"MC Price = {mc_price:.6f}\n"
        f"ML Price = {ml_price:.6f}\n"
        f"Error = {relative_error:.4f}%"
    )
    ax4.text(0.1, 0.9, param_text, transform=ax4.transAxes, fontsize=12,
             verticalalignment='top', fontfamily='monospace', bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))

    plt.suptitle(f'Chooser Option Pricing Prediction\n(Relative Error: {relative_error:.4f}%)', 
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

    return {
        'monte_carlo_price': mc_price,
        'ml_predicted_price': ml_price,
        'relative_error_percent': relative_error,
        'monte_carlo_std': mc_std,
        'ml_std': ml_std,
        'total_time': t3 - t1
    }

# ==========================================================
# 9ï¸âƒ£ å‘½ä»¤è¡Œæ¥å£ä¸»å‡½æ•°
# ==========================================================

def main():
    """å‘½ä»¤è¡Œä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒã€è¯„ä¼°å’Œé¢„æµ‹é€‰æ‹©æœŸæƒå®šä»·æ¨¡å‹')
    
    parser.add_argument('--mode', '-m', type=str, required=True,
                       choices=['train', 'eval', 'predict'],
                       help='è¿è¡Œæ¨¡å¼: train (è®­ç»ƒ), eval (è¯„ä¼°), predict (é¢„æµ‹)')
    
    parser.add_argument('--data_type', '-d', type=str, 
                       choices=['synthetic', 'historical', 'mixed'],
                       help='æ•°æ®ç±»å‹: synthetic (åˆæˆæ•°æ®), historical (å†å²æ•°æ®), mixed (æ··åˆæ•°æ®)')
    
    parser.add_argument('--enhanced', '-e', action='store_true',
                       help='æ˜¯å¦ä½¿ç”¨å¢å¼ºå†å²æ•°æ®ï¼ˆæ·»åŠ æ‰°åŠ¨ï¼‰')
    
    parser.add_argument('--n_total_paths', '-n', type=int, default=400000,
                       help='æ€»è·¯å¾„æ•° (é»˜è®¤: 400000)')
    
    parser.add_argument('--model_path', type=str, default=None,
                       help='æ¨¡å‹è·¯å¾„ (ç”¨äºè¯„ä¼°æˆ–é¢„æµ‹æ¨¡å¼)')
    
    parser.add_argument('--eval_paths', type=int, default=500000,
                       help='è¯„ä¼°æ—¶ä½¿ç”¨çš„è·¯å¾„æ•° (é»˜è®¤: 500000)')
    
    parser.add_argument('--scaler_path', type=str, default=None,
                       help='Scaleræ–‡ä»¶è·¯å¾„ (ç”¨äºè¯„ä¼°æˆ–é¢„æµ‹æ¨¡å¼ï¼Œå¦‚ä¸æŒ‡å®šåˆ™è‡ªåŠ¨æ¨æ–­)')
    
    # é¢„æµ‹æ¨¡å¼ä¸“ç”¨å‚æ•°
    parser.add_argument('--s0', type=float, help='åˆå§‹è‚¡ä»·')
    parser.add_argument('--K', type=float, help='æ‰§è¡Œä»·')
    parser.add_argument('--r', type=float, help='æ— é£é™©åˆ©ç‡')
    parser.add_argument('--q', type=float, help='è‚¡æ¯ç‡')
    parser.add_argument('--sigma', type=float, help='æ³¢åŠ¨ç‡')
    parser.add_argument('--T', type=float, help='åˆ°æœŸæ—¶é—´')
    parser.add_argument('--choice_date', type=float, help='é€‰æ‹©æ—¥æœŸ')
    parser.add_argument('--n_paths_predict', type=int, default=100000,
                       help='é¢„æµ‹æ—¶ä½¿ç”¨çš„è·¯å¾„æ•° (é»˜è®¤: 100000)')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ å¼€å§‹è¿è¡Œé€‰æ‹©æœŸæƒå®šä»·æ¨¡å‹")
    print(f"ğŸ“Š æ¨¡å¼: {args.mode}")
    
    if args.mode == 'train':
        if args.data_type is None:
            print("âŒ è®­ç»ƒæ¨¡å¼å¿…é¡»æŒ‡å®š --data_type å‚æ•°")
            sys.exit(1)
            
        # æ˜ å°„æ•°æ®ç±»å‹çš„å‚æ•°
        data_mode_mapping = {
            'synthetic': 'synthetic_only',
            'historical': 'historical_only', 
            'mixed': 'mixed'
        }
        
        data_mode = data_mode_mapping.get(args.data_type, 'mixed')
        
        print(f"ğŸ“ˆ æ•°æ®ç±»å‹: {args.data_type} -> {data_mode}")
        print(f"ğŸ”§ å¢å¼ºæ¨¡å¼: {args.enhanced}")
        print(f"ğŸ“Š æ€»è·¯å¾„æ•°: {args.n_total_paths}")
        
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å¼...")
        model, scaler, history = train_model(
            data_type=data_mode,
            enhanced=args.enhanced,
            n_total_paths=args.n_total_paths
        )
        
        if model is not None:
            print("âœ… è®­ç»ƒå®Œæˆ!")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥!")
            sys.exit(1)
            
    elif args.mode == 'eval':
        if args.model_path is None and args.data_type is None:
            print("âŒ è¯„ä¼°æ¨¡å¼å¿…é¡»æŒ‡å®š --model_path æˆ– --data_type å‚æ•°")
            sys.exit(1)
            
        if args.model_path is None:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤å‘½åè§„åˆ™
            args.model_path = f'models/chooser_option_mlp_model_{args.data_type}.h5'
            print(f"ğŸ” ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„: {args.model_path}")
        
                # å¦‚æœæ‰‹åŠ¨æŒ‡å®šäº†scalerè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨
        if args.scaler_path:
            scaler_path = args.scaler_path
        else:
            scaler_path = None  # è®©å‡½æ•°è‡ªåŠ¨æ¨æ–­

        print("\nğŸ“Š å¼€å§‹è¯„ä¼°æ¨¡å¼...")
        results = evaluate_model(
            model_path=args.model_path,
            data_type=args.data_type if args.data_type else 'mixed',
            n_paths=args.eval_paths
        )
        
        if results is not None:
            print("âœ… è¯„ä¼°å®Œæˆ!")
            print(f"ğŸ“‹ è¯„ä¼°ç»“æœ:")
            for metric, value in results.items():
                print(f"  {metric}: {value:.4f}")
        else:
            print("âŒ è¯„ä¼°å¤±è´¥!")
            sys.exit(1)
        
    elif args.mode == 'predict':
        # æ£€æŸ¥å¿…éœ€çš„é¢„æµ‹å‚æ•°
        required_params = ['s0', 'K', 'r', 'q', 'sigma', 'T', 'choice_date']
        missing_params = [param for param in required_params if getattr(args, param) is None]
        
        if missing_params:
            print(f"âŒ é¢„æµ‹æ¨¡å¼ç¼ºå°‘å¿…éœ€å‚æ•°: {', '.join(missing_params)}")
            print("â„¹ï¸  é¢„æµ‹æ¨¡å¼éœ€è¦ä»¥ä¸‹å‚æ•°:")
            print("  --s0: åˆå§‹è‚¡ä»·")
            print("  --K: æ‰§è¡Œä»·") 
            print("  --r: æ— é£é™©åˆ©ç‡")
            print("  --q: è‚¡æ¯ç‡")
            print("  --sigma: æ³¢åŠ¨ç‡")
            print("  --T: åˆ°æœŸæ—¶é—´")
            print("  --choice_date: é€‰æ‹©æ—¥æœŸ")
            sys.exit(1)
            
        if args.model_path is None:
            print("âŒ é¢„æµ‹æ¨¡å¼å¿…é¡»æŒ‡å®š --model_path å‚æ•°")
            sys.exit(1)

        # å¦‚æœæ‰‹åŠ¨æŒ‡å®šäº†scalerè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨
        if args.scaler_path:
            scaler_path = args.scaler_path
        else:
            scaler_path = None  # è®©å‡½æ•°è‡ªåŠ¨æ¨æ–­
        
        results = predict_option_price(
            model_path=args.model_path,
            s0=args.s0,
            K=args.K,
            r=args.r,
            q=args.q,
            sigma=args.sigma,
            T=args.T,
            choice_date=args.choice_date,
            n_paths=args.n_paths_predict
        )
        
        if results is not None:
            print("âœ… é¢„æµ‹å®Œæˆ!")
        else:
            print("âŒ é¢„æµ‹å¤±è´¥!")
            sys.exit(1)
    
    print("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")

if __name__ == '__main__':
    main()
